{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgrDNH41VYKDo3J1l1BTI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skiraware/BabyGPT/blob/main/babygpt_refined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foBvJO_bQMAO",
        "outputId": "f08d554d-fa2f-4502-9a52-6af6b38ef791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.03 KiB | 9.54 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n",
            "/content/nanoGPT\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT\n",
        "%cd nanoGPT\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB_IoUkAQlNf",
        "outputId": "d1734bf5-9fa1-401f-c3ca-52090a4742b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py  --compile=False --always_save_checkpoint=True --eval_interval=5000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d7Q181MRl86",
        "outputId": "734ddfe0-1de5-47bf-ee1b-6e7f353dbd25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "Overriding: always_save_checkpoint = True\n",
            "Overriding: eval_interval = 5000\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2664, time 70461.27ms, mfu -100.00%\n",
            "iter 10: loss 3.1421, time 514.61ms, mfu 0.72%\n",
            "iter 20: loss 2.7345, time 515.68ms, mfu 0.72%\n",
            "iter 30: loss 2.6178, time 516.99ms, mfu 0.72%\n",
            "iter 40: loss 2.5734, time 519.53ms, mfu 0.72%\n",
            "iter 50: loss 2.5265, time 523.94ms, mfu 0.72%\n",
            "iter 60: loss 2.5110, time 522.53ms, mfu 0.72%\n",
            "iter 70: loss 2.4956, time 527.79ms, mfu 0.72%\n",
            "iter 80: loss 2.4940, time 527.50ms, mfu 0.72%\n",
            "iter 90: loss 2.4703, time 530.16ms, mfu 0.72%\n",
            "iter 100: loss 2.4566, time 532.36ms, mfu 0.71%\n",
            "iter 110: loss 2.4512, time 534.86ms, mfu 0.71%\n",
            "iter 120: loss 2.4283, time 535.59ms, mfu 0.71%\n",
            "iter 130: loss 2.4101, time 535.60ms, mfu 0.71%\n",
            "iter 140: loss 2.4184, time 536.79ms, mfu 0.71%\n",
            "iter 150: loss 2.4237, time 533.39ms, mfu 0.71%\n",
            "iter 160: loss 2.3839, time 535.97ms, mfu 0.71%\n",
            "iter 170: loss 2.3574, time 529.51ms, mfu 0.71%\n",
            "iter 180: loss 2.3153, time 531.56ms, mfu 0.71%\n",
            "iter 190: loss 2.2506, time 530.03ms, mfu 0.71%\n",
            "iter 200: loss 2.2119, time 527.70ms, mfu 0.71%\n",
            "iter 210: loss 2.1391, time 529.91ms, mfu 0.71%\n",
            "iter 220: loss 2.1315, time 528.55ms, mfu 0.71%\n",
            "iter 230: loss 2.0739, time 528.41ms, mfu 0.71%\n",
            "iter 240: loss 2.0785, time 527.77ms, mfu 0.71%\n",
            "iter 250: loss 2.0310, time 531.10ms, mfu 0.70%\n",
            "iter 260: loss 2.0057, time 530.64ms, mfu 0.70%\n",
            "iter 270: loss 1.9892, time 531.13ms, mfu 0.70%\n",
            "iter 280: loss 1.9638, time 531.35ms, mfu 0.70%\n",
            "iter 290: loss 1.9407, time 532.58ms, mfu 0.70%\n",
            "iter 300: loss 1.8796, time 531.11ms, mfu 0.70%\n",
            "iter 310: loss 1.8864, time 531.74ms, mfu 0.70%\n",
            "iter 320: loss 1.8475, time 531.60ms, mfu 0.70%\n",
            "iter 330: loss 1.8394, time 531.30ms, mfu 0.70%\n",
            "iter 340: loss 1.8182, time 529.69ms, mfu 0.70%\n",
            "iter 350: loss 1.7861, time 531.09ms, mfu 0.70%\n",
            "iter 360: loss 1.7847, time 529.56ms, mfu 0.70%\n",
            "iter 370: loss 1.7667, time 532.14ms, mfu 0.70%\n",
            "iter 380: loss 1.7379, time 533.59ms, mfu 0.70%\n",
            "iter 390: loss 1.7091, time 532.90ms, mfu 0.70%\n",
            "iter 400: loss 1.7333, time 531.43ms, mfu 0.70%\n",
            "iter 410: loss 1.7035, time 532.14ms, mfu 0.70%\n",
            "iter 420: loss 1.7049, time 532.57ms, mfu 0.70%\n",
            "iter 430: loss 1.6587, time 533.11ms, mfu 0.70%\n",
            "iter 440: loss 1.6786, time 530.25ms, mfu 0.70%\n",
            "iter 450: loss 1.6589, time 531.30ms, mfu 0.70%\n",
            "iter 460: loss 1.6842, time 530.64ms, mfu 0.70%\n",
            "iter 470: loss 1.6638, time 531.84ms, mfu 0.70%\n",
            "iter 480: loss 1.6144, time 531.35ms, mfu 0.70%\n",
            "iter 490: loss 1.6286, time 532.04ms, mfu 0.70%\n",
            "iter 500: loss 1.6130, time 533.36ms, mfu 0.70%\n",
            "iter 510: loss 1.5733, time 531.67ms, mfu 0.70%\n",
            "iter 520: loss 1.5862, time 532.24ms, mfu 0.70%\n",
            "iter 530: loss 1.5716, time 531.02ms, mfu 0.70%\n",
            "iter 540: loss 1.5806, time 530.49ms, mfu 0.70%\n",
            "iter 550: loss 1.5687, time 531.73ms, mfu 0.70%\n",
            "iter 560: loss 1.5400, time 533.66ms, mfu 0.70%\n",
            "iter 570: loss 1.5781, time 532.62ms, mfu 0.70%\n",
            "iter 580: loss 1.5403, time 533.00ms, mfu 0.70%\n",
            "iter 590: loss 1.5475, time 529.55ms, mfu 0.70%\n",
            "iter 600: loss 1.5309, time 533.56ms, mfu 0.70%\n",
            "iter 610: loss 1.5355, time 532.81ms, mfu 0.70%\n",
            "iter 620: loss 1.5336, time 531.45ms, mfu 0.70%\n",
            "iter 630: loss 1.5105, time 531.57ms, mfu 0.70%\n",
            "iter 640: loss 1.5012, time 532.59ms, mfu 0.70%\n",
            "iter 650: loss 1.4844, time 531.74ms, mfu 0.70%\n",
            "iter 660: loss 1.4787, time 533.36ms, mfu 0.70%\n",
            "iter 670: loss 1.4797, time 531.17ms, mfu 0.70%\n",
            "iter 680: loss 1.4943, time 531.73ms, mfu 0.70%\n",
            "iter 690: loss 1.4703, time 531.60ms, mfu 0.70%\n",
            "iter 700: loss 1.4624, time 531.81ms, mfu 0.70%\n",
            "iter 710: loss 1.4410, time 531.07ms, mfu 0.70%\n",
            "iter 720: loss 1.4667, time 531.20ms, mfu 0.70%\n",
            "iter 730: loss 1.4290, time 532.58ms, mfu 0.70%\n",
            "iter 740: loss 1.4222, time 531.30ms, mfu 0.70%\n",
            "iter 750: loss 1.4631, time 532.49ms, mfu 0.70%\n",
            "iter 760: loss 1.4321, time 532.18ms, mfu 0.70%\n",
            "iter 770: loss 1.4034, time 532.93ms, mfu 0.70%\n",
            "iter 780: loss 1.4276, time 532.73ms, mfu 0.70%\n",
            "iter 790: loss 1.4186, time 530.98ms, mfu 0.70%\n",
            "iter 800: loss 1.4542, time 531.74ms, mfu 0.70%\n",
            "iter 810: loss 1.4026, time 530.66ms, mfu 0.70%\n",
            "iter 820: loss 1.4318, time 531.94ms, mfu 0.70%\n",
            "iter 830: loss 1.3953, time 531.47ms, mfu 0.70%\n",
            "iter 840: loss 1.3994, time 532.03ms, mfu 0.70%\n",
            "iter 850: loss 1.3896, time 531.46ms, mfu 0.70%\n",
            "iter 860: loss 1.3490, time 531.31ms, mfu 0.70%\n",
            "iter 870: loss 1.4072, time 531.88ms, mfu 0.70%\n",
            "iter 880: loss 1.3851, time 532.45ms, mfu 0.70%\n",
            "iter 890: loss 1.3707, time 531.28ms, mfu 0.70%\n",
            "iter 900: loss 1.3732, time 529.64ms, mfu 0.70%\n",
            "iter 910: loss 1.3791, time 532.05ms, mfu 0.70%\n",
            "iter 920: loss 1.3574, time 530.22ms, mfu 0.70%\n",
            "iter 930: loss 1.3840, time 531.84ms, mfu 0.70%\n",
            "iter 940: loss 1.3588, time 532.23ms, mfu 0.70%\n",
            "iter 950: loss 1.3727, time 529.47ms, mfu 0.70%\n",
            "iter 960: loss 1.3658, time 532.57ms, mfu 0.70%\n",
            "iter 970: loss 1.3988, time 531.09ms, mfu 0.70%\n",
            "iter 980: loss 1.3753, time 532.77ms, mfu 0.70%\n",
            "iter 990: loss 1.3217, time 531.83ms, mfu 0.70%\n",
            "iter 1000: loss 1.3568, time 531.49ms, mfu 0.70%\n",
            "iter 1010: loss 1.3128, time 531.14ms, mfu 0.70%\n",
            "iter 1020: loss 1.3393, time 530.50ms, mfu 0.70%\n",
            "iter 1030: loss 1.3271, time 530.11ms, mfu 0.70%\n",
            "iter 1040: loss 1.3149, time 530.60ms, mfu 0.70%\n",
            "iter 1050: loss 1.3465, time 531.46ms, mfu 0.70%\n",
            "iter 1060: loss 1.3513, time 531.25ms, mfu 0.70%\n",
            "iter 1070: loss 1.2903, time 533.21ms, mfu 0.70%\n",
            "iter 1080: loss 1.3567, time 533.78ms, mfu 0.70%\n",
            "iter 1090: loss 1.3305, time 532.39ms, mfu 0.70%\n",
            "iter 1100: loss 1.3370, time 531.68ms, mfu 0.70%\n",
            "iter 1110: loss 1.3015, time 531.07ms, mfu 0.70%\n",
            "iter 1120: loss 1.2944, time 530.04ms, mfu 0.70%\n",
            "iter 1130: loss 1.2928, time 530.92ms, mfu 0.70%\n",
            "iter 1140: loss 1.3176, time 531.75ms, mfu 0.70%\n",
            "iter 1150: loss 1.3089, time 532.22ms, mfu 0.70%\n",
            "iter 1160: loss 1.2766, time 530.99ms, mfu 0.70%\n",
            "iter 1170: loss 1.2876, time 532.57ms, mfu 0.70%\n",
            "iter 1180: loss 1.2812, time 531.22ms, mfu 0.70%\n",
            "iter 1190: loss 1.2983, time 531.85ms, mfu 0.70%\n",
            "iter 1200: loss 1.3133, time 533.02ms, mfu 0.70%\n",
            "iter 1210: loss 1.2936, time 532.19ms, mfu 0.70%\n",
            "iter 1220: loss 1.2884, time 532.56ms, mfu 0.70%\n",
            "iter 1230: loss 1.2813, time 531.22ms, mfu 0.70%\n",
            "iter 1240: loss 1.2810, time 530.92ms, mfu 0.70%\n",
            "iter 1250: loss 1.2865, time 530.44ms, mfu 0.70%\n",
            "iter 1260: loss 1.2907, time 531.02ms, mfu 0.70%\n",
            "iter 1270: loss 1.2809, time 530.80ms, mfu 0.70%\n",
            "iter 1280: loss 1.2361, time 531.51ms, mfu 0.70%\n",
            "iter 1290: loss 1.2884, time 530.04ms, mfu 0.70%\n",
            "iter 1300: loss 1.2616, time 531.18ms, mfu 0.70%\n",
            "iter 1310: loss 1.2914, time 529.50ms, mfu 0.70%\n",
            "iter 1320: loss 1.2741, time 530.40ms, mfu 0.70%\n",
            "iter 1330: loss 1.2506, time 531.46ms, mfu 0.70%\n",
            "iter 1340: loss 1.3010, time 531.05ms, mfu 0.70%\n",
            "iter 1350: loss 1.2714, time 532.98ms, mfu 0.70%\n",
            "iter 1360: loss 1.2745, time 531.13ms, mfu 0.70%\n",
            "iter 1370: loss 1.2660, time 532.53ms, mfu 0.70%\n",
            "iter 1380: loss 1.2334, time 531.98ms, mfu 0.70%\n",
            "iter 1390: loss 1.2144, time 533.28ms, mfu 0.70%\n",
            "iter 1400: loss 1.2433, time 531.90ms, mfu 0.70%\n",
            "iter 1410: loss 1.2728, time 532.31ms, mfu 0.70%\n",
            "iter 1420: loss 1.2746, time 532.21ms, mfu 0.70%\n",
            "iter 1430: loss 1.2436, time 530.04ms, mfu 0.70%\n",
            "iter 1440: loss 1.2272, time 529.21ms, mfu 0.70%\n",
            "iter 1450: loss 1.2509, time 530.68ms, mfu 0.70%\n",
            "iter 1460: loss 1.2547, time 531.38ms, mfu 0.70%\n",
            "iter 1470: loss 1.2147, time 530.43ms, mfu 0.70%\n",
            "iter 1480: loss 1.2657, time 532.26ms, mfu 0.70%\n",
            "iter 1490: loss 1.2307, time 536.85ms, mfu 0.70%\n",
            "iter 1500: loss 1.2403, time 533.66ms, mfu 0.70%\n",
            "iter 1510: loss 1.2281, time 532.99ms, mfu 0.70%\n",
            "iter 1520: loss 1.2311, time 532.54ms, mfu 0.70%\n",
            "iter 1530: loss 1.2109, time 533.55ms, mfu 0.70%\n",
            "iter 1540: loss 1.2161, time 532.17ms, mfu 0.70%\n",
            "iter 1550: loss 1.2191, time 530.36ms, mfu 0.70%\n",
            "iter 1560: loss 1.2257, time 529.86ms, mfu 0.70%\n",
            "iter 1570: loss 1.2178, time 532.97ms, mfu 0.70%\n",
            "iter 1580: loss 1.2219, time 532.57ms, mfu 0.70%\n",
            "iter 1590: loss 1.2410, time 531.63ms, mfu 0.70%\n",
            "iter 1600: loss 1.2175, time 530.06ms, mfu 0.70%\n",
            "iter 1610: loss 1.2330, time 530.34ms, mfu 0.70%\n",
            "iter 1620: loss 1.2152, time 531.05ms, mfu 0.70%\n",
            "iter 1630: loss 1.1778, time 530.33ms, mfu 0.70%\n",
            "iter 1640: loss 1.2141, time 529.33ms, mfu 0.70%\n",
            "iter 1650: loss 1.2141, time 529.65ms, mfu 0.70%\n",
            "iter 1660: loss 1.1873, time 531.41ms, mfu 0.70%\n",
            "iter 1670: loss 1.1756, time 530.71ms, mfu 0.70%\n",
            "iter 1680: loss 1.1951, time 529.16ms, mfu 0.70%\n",
            "iter 1690: loss 1.1499, time 530.36ms, mfu 0.70%\n",
            "iter 1700: loss 1.2090, time 530.72ms, mfu 0.70%\n",
            "iter 1710: loss 1.2094, time 528.50ms, mfu 0.70%\n",
            "iter 1720: loss 1.1894, time 533.47ms, mfu 0.70%\n",
            "iter 1730: loss 1.1873, time 530.43ms, mfu 0.70%\n",
            "iter 1740: loss 1.1501, time 529.87ms, mfu 0.70%\n",
            "iter 1750: loss 1.1867, time 529.79ms, mfu 0.70%\n",
            "iter 1760: loss 1.1745, time 532.31ms, mfu 0.70%\n",
            "iter 1770: loss 1.1617, time 530.89ms, mfu 0.70%\n",
            "iter 1780: loss 1.1909, time 533.38ms, mfu 0.70%\n",
            "iter 1790: loss 1.1768, time 530.87ms, mfu 0.70%\n",
            "iter 1800: loss 1.1517, time 532.21ms, mfu 0.70%\n",
            "iter 1810: loss 1.1660, time 534.23ms, mfu 0.70%\n",
            "iter 1820: loss 1.1812, time 531.83ms, mfu 0.70%\n",
            "iter 1830: loss 1.1654, time 531.47ms, mfu 0.70%\n",
            "iter 1840: loss 1.1610, time 531.75ms, mfu 0.70%\n",
            "iter 1850: loss 1.1786, time 531.31ms, mfu 0.70%\n",
            "iter 1860: loss 1.1760, time 532.79ms, mfu 0.70%\n",
            "iter 1870: loss 1.1618, time 531.28ms, mfu 0.70%\n",
            "iter 1880: loss 1.1371, time 532.05ms, mfu 0.70%\n",
            "iter 1890: loss 1.1627, time 532.47ms, mfu 0.70%\n",
            "iter 1900: loss 1.1729, time 534.63ms, mfu 0.70%\n",
            "iter 1910: loss 1.1792, time 531.78ms, mfu 0.70%\n",
            "iter 1920: loss 1.1694, time 532.97ms, mfu 0.70%\n",
            "iter 1930: loss 1.1626, time 533.38ms, mfu 0.70%\n",
            "iter 1940: loss 1.1501, time 533.88ms, mfu 0.70%\n",
            "iter 1950: loss 1.1680, time 533.70ms, mfu 0.70%\n",
            "iter 1960: loss 1.1855, time 531.16ms, mfu 0.70%\n",
            "iter 1970: loss 1.1558, time 530.43ms, mfu 0.70%\n",
            "iter 1980: loss 1.1522, time 534.19ms, mfu 0.70%\n",
            "iter 1990: loss 1.1376, time 531.20ms, mfu 0.70%\n",
            "iter 2000: loss 1.1631, time 532.49ms, mfu 0.70%\n",
            "iter 2010: loss 1.1461, time 533.42ms, mfu 0.70%\n",
            "iter 2020: loss 1.1349, time 532.24ms, mfu 0.70%\n",
            "iter 2030: loss 1.1374, time 530.91ms, mfu 0.70%\n",
            "iter 2040: loss 1.1387, time 531.47ms, mfu 0.70%\n",
            "iter 2050: loss 1.1381, time 531.89ms, mfu 0.70%\n",
            "iter 2060: loss 1.1427, time 533.28ms, mfu 0.70%\n",
            "iter 2070: loss 1.1400, time 532.08ms, mfu 0.70%\n",
            "iter 2080: loss 1.1233, time 531.26ms, mfu 0.70%\n",
            "iter 2090: loss 1.1492, time 531.94ms, mfu 0.70%\n",
            "iter 2100: loss 1.1143, time 531.14ms, mfu 0.70%\n",
            "iter 2110: loss 1.0885, time 533.28ms, mfu 0.70%\n",
            "iter 2120: loss 1.1257, time 533.49ms, mfu 0.70%\n",
            "iter 2130: loss 1.1319, time 531.78ms, mfu 0.70%\n",
            "iter 2140: loss 1.1145, time 530.92ms, mfu 0.70%\n",
            "iter 2150: loss 1.1201, time 529.93ms, mfu 0.70%\n",
            "iter 2160: loss 1.1344, time 531.19ms, mfu 0.70%\n",
            "iter 2170: loss 1.1203, time 530.31ms, mfu 0.70%\n",
            "iter 2180: loss 1.1196, time 531.87ms, mfu 0.70%\n",
            "iter 2190: loss 1.1056, time 531.96ms, mfu 0.70%\n",
            "iter 2200: loss 1.1134, time 530.35ms, mfu 0.70%\n",
            "iter 2210: loss 1.1067, time 535.36ms, mfu 0.70%\n",
            "iter 2220: loss 1.1025, time 533.49ms, mfu 0.70%\n",
            "iter 2230: loss 1.1113, time 533.36ms, mfu 0.70%\n",
            "iter 2240: loss 1.1039, time 532.59ms, mfu 0.70%\n",
            "iter 2250: loss 1.1108, time 534.68ms, mfu 0.70%\n",
            "iter 2260: loss 1.1090, time 534.05ms, mfu 0.70%\n",
            "iter 2270: loss 1.0933, time 531.71ms, mfu 0.70%\n",
            "iter 2280: loss 1.1013, time 530.01ms, mfu 0.70%\n",
            "iter 2290: loss 1.1134, time 532.36ms, mfu 0.70%\n",
            "iter 2300: loss 1.1019, time 530.61ms, mfu 0.70%\n",
            "iter 2310: loss 1.1121, time 531.27ms, mfu 0.70%\n",
            "iter 2320: loss 1.0924, time 530.40ms, mfu 0.70%\n",
            "iter 2330: loss 1.1207, time 532.14ms, mfu 0.70%\n",
            "iter 2340: loss 1.0803, time 531.18ms, mfu 0.70%\n",
            "iter 2350: loss 1.0793, time 532.39ms, mfu 0.70%\n",
            "iter 2360: loss 1.0896, time 531.92ms, mfu 0.70%\n",
            "iter 2370: loss 1.0861, time 530.26ms, mfu 0.70%\n",
            "iter 2380: loss 1.0838, time 532.47ms, mfu 0.70%\n",
            "iter 2390: loss 1.0784, time 531.62ms, mfu 0.70%\n",
            "iter 2400: loss 1.0970, time 533.67ms, mfu 0.70%\n",
            "iter 2410: loss 1.0816, time 534.08ms, mfu 0.70%\n",
            "iter 2420: loss 1.0621, time 531.16ms, mfu 0.70%\n",
            "iter 2430: loss 1.0611, time 532.04ms, mfu 0.70%\n",
            "iter 2440: loss 1.0801, time 533.14ms, mfu 0.70%\n",
            "iter 2450: loss 1.0669, time 531.69ms, mfu 0.70%\n",
            "iter 2460: loss 1.0712, time 533.26ms, mfu 0.70%\n",
            "iter 2470: loss 1.1081, time 534.00ms, mfu 0.70%\n",
            "iter 2480: loss 1.0652, time 533.35ms, mfu 0.70%\n",
            "iter 2490: loss 1.0793, time 531.77ms, mfu 0.70%\n",
            "iter 2500: loss 1.0918, time 532.77ms, mfu 0.70%\n",
            "iter 2510: loss 1.0803, time 533.02ms, mfu 0.70%\n",
            "iter 2520: loss 1.0598, time 532.94ms, mfu 0.70%\n",
            "iter 2530: loss 1.0714, time 532.03ms, mfu 0.70%\n",
            "iter 2540: loss 1.0644, time 531.85ms, mfu 0.70%\n",
            "iter 2550: loss 1.0634, time 534.01ms, mfu 0.70%\n",
            "iter 2560: loss 1.0488, time 533.08ms, mfu 0.70%\n",
            "iter 2570: loss 1.0495, time 530.92ms, mfu 0.70%\n",
            "iter 2580: loss 1.0815, time 532.96ms, mfu 0.70%\n",
            "iter 2590: loss 1.0867, time 531.80ms, mfu 0.70%\n",
            "iter 2600: loss 1.0803, time 532.07ms, mfu 0.70%\n",
            "iter 2610: loss 1.0374, time 532.75ms, mfu 0.70%\n",
            "iter 2620: loss 1.0302, time 530.52ms, mfu 0.70%\n",
            "iter 2630: loss 1.0489, time 532.74ms, mfu 0.70%\n",
            "iter 2640: loss 1.0859, time 530.72ms, mfu 0.70%\n",
            "iter 2650: loss 1.0197, time 532.13ms, mfu 0.70%\n",
            "iter 2660: loss 1.0462, time 533.41ms, mfu 0.70%\n",
            "iter 2670: loss 1.0560, time 530.06ms, mfu 0.70%\n",
            "iter 2680: loss 1.0533, time 529.47ms, mfu 0.70%\n",
            "iter 2690: loss 1.0676, time 532.19ms, mfu 0.70%\n",
            "iter 2700: loss 1.0383, time 531.39ms, mfu 0.70%\n",
            "iter 2710: loss 1.0352, time 532.32ms, mfu 0.70%\n",
            "iter 2720: loss 1.0257, time 531.98ms, mfu 0.70%\n",
            "iter 2730: loss 1.0373, time 531.70ms, mfu 0.70%\n",
            "iter 2740: loss 1.0298, time 531.13ms, mfu 0.70%\n",
            "iter 2750: loss 1.0387, time 531.59ms, mfu 0.70%\n",
            "iter 2760: loss 1.0424, time 531.63ms, mfu 0.70%\n",
            "iter 2770: loss 1.0352, time 530.25ms, mfu 0.70%\n",
            "iter 2780: loss 1.0412, time 530.43ms, mfu 0.70%\n",
            "iter 2790: loss 1.0146, time 529.81ms, mfu 0.70%\n",
            "iter 2800: loss 1.0327, time 532.25ms, mfu 0.70%\n",
            "iter 2810: loss 1.0010, time 529.35ms, mfu 0.70%\n",
            "iter 2820: loss 1.0348, time 530.12ms, mfu 0.70%\n",
            "iter 2830: loss 1.0200, time 532.14ms, mfu 0.70%\n",
            "iter 2840: loss 1.0362, time 531.36ms, mfu 0.70%\n",
            "iter 2850: loss 1.0193, time 532.06ms, mfu 0.70%\n",
            "iter 2860: loss 1.0318, time 531.25ms, mfu 0.70%\n",
            "iter 2870: loss 1.0168, time 530.88ms, mfu 0.70%\n",
            "iter 2880: loss 1.0194, time 530.10ms, mfu 0.70%\n",
            "iter 2890: loss 1.0432, time 531.77ms, mfu 0.70%\n",
            "iter 2900: loss 1.0145, time 530.71ms, mfu 0.70%\n",
            "iter 2910: loss 0.9974, time 529.80ms, mfu 0.70%\n",
            "iter 2920: loss 1.0020, time 530.73ms, mfu 0.70%\n",
            "iter 2930: loss 1.0034, time 528.26ms, mfu 0.70%\n",
            "iter 2940: loss 1.0025, time 530.17ms, mfu 0.70%\n",
            "iter 2950: loss 0.9877, time 530.61ms, mfu 0.70%\n",
            "iter 2960: loss 1.0052, time 530.75ms, mfu 0.70%\n",
            "iter 2970: loss 0.9955, time 532.38ms, mfu 0.70%\n",
            "iter 2980: loss 1.0076, time 530.77ms, mfu 0.70%\n",
            "iter 2990: loss 1.0086, time 530.84ms, mfu 0.70%\n",
            "iter 3000: loss 0.9818, time 530.47ms, mfu 0.70%\n",
            "iter 3010: loss 1.0043, time 530.33ms, mfu 0.70%\n",
            "iter 3020: loss 0.9915, time 529.32ms, mfu 0.70%\n",
            "iter 3030: loss 0.9885, time 530.97ms, mfu 0.70%\n",
            "iter 3040: loss 0.9706, time 530.34ms, mfu 0.70%\n",
            "iter 3050: loss 1.0084, time 532.60ms, mfu 0.70%\n",
            "iter 3060: loss 0.9792, time 530.37ms, mfu 0.70%\n",
            "iter 3070: loss 0.9974, time 530.63ms, mfu 0.70%\n",
            "iter 3080: loss 1.0249, time 531.37ms, mfu 0.70%\n",
            "iter 3090: loss 0.9790, time 531.21ms, mfu 0.70%\n",
            "iter 3100: loss 0.9982, time 531.55ms, mfu 0.70%\n",
            "iter 3110: loss 1.0147, time 531.71ms, mfu 0.70%\n",
            "iter 3120: loss 0.9716, time 532.64ms, mfu 0.70%\n",
            "iter 3130: loss 1.0127, time 531.18ms, mfu 0.70%\n",
            "iter 3140: loss 0.9739, time 531.40ms, mfu 0.70%\n",
            "iter 3150: loss 0.9945, time 531.07ms, mfu 0.70%\n",
            "iter 3160: loss 0.9904, time 531.96ms, mfu 0.70%\n",
            "iter 3170: loss 0.9619, time 531.57ms, mfu 0.70%\n",
            "iter 3180: loss 0.9709, time 530.56ms, mfu 0.70%\n",
            "iter 3190: loss 0.9456, time 530.58ms, mfu 0.70%\n",
            "iter 3200: loss 1.0065, time 532.94ms, mfu 0.70%\n",
            "iter 3210: loss 0.9550, time 529.60ms, mfu 0.70%\n",
            "iter 3220: loss 0.9710, time 531.46ms, mfu 0.70%\n",
            "iter 3230: loss 0.9882, time 531.41ms, mfu 0.70%\n",
            "iter 3240: loss 0.9594, time 532.04ms, mfu 0.70%\n",
            "iter 3250: loss 0.9902, time 532.47ms, mfu 0.70%\n",
            "iter 3260: loss 0.9712, time 531.11ms, mfu 0.70%\n",
            "iter 3270: loss 0.9553, time 530.21ms, mfu 0.70%\n",
            "iter 3280: loss 0.9512, time 529.57ms, mfu 0.70%\n",
            "iter 3290: loss 0.9997, time 531.52ms, mfu 0.70%\n",
            "iter 3300: loss 0.9923, time 530.46ms, mfu 0.70%\n",
            "iter 3310: loss 0.9429, time 533.07ms, mfu 0.70%\n",
            "iter 3320: loss 1.0040, time 530.95ms, mfu 0.70%\n",
            "iter 3330: loss 0.9553, time 532.44ms, mfu 0.70%\n",
            "iter 3340: loss 0.9869, time 532.66ms, mfu 0.70%\n",
            "iter 3350: loss 0.9549, time 531.31ms, mfu 0.70%\n",
            "iter 3360: loss 0.9598, time 532.91ms, mfu 0.70%\n",
            "iter 3370: loss 0.9554, time 534.43ms, mfu 0.70%\n",
            "iter 3380: loss 0.9556, time 531.76ms, mfu 0.70%\n",
            "iter 3390: loss 0.9559, time 533.27ms, mfu 0.70%\n",
            "iter 3400: loss 0.9612, time 533.80ms, mfu 0.70%\n",
            "iter 3410: loss 0.9668, time 531.24ms, mfu 0.70%\n",
            "iter 3420: loss 0.9768, time 531.27ms, mfu 0.70%\n",
            "iter 3430: loss 0.9496, time 531.97ms, mfu 0.70%\n",
            "iter 3440: loss 0.9549, time 529.88ms, mfu 0.70%\n",
            "iter 3450: loss 0.9412, time 531.41ms, mfu 0.70%\n",
            "iter 3460: loss 0.9541, time 530.81ms, mfu 0.70%\n",
            "iter 3470: loss 0.9337, time 531.14ms, mfu 0.70%\n",
            "iter 3480: loss 0.9229, time 532.73ms, mfu 0.70%\n",
            "iter 3490: loss 0.9384, time 530.90ms, mfu 0.70%\n",
            "iter 3500: loss 0.8949, time 531.92ms, mfu 0.70%\n",
            "iter 3510: loss 0.9419, time 530.34ms, mfu 0.70%\n",
            "iter 3520: loss 0.9555, time 532.09ms, mfu 0.70%\n",
            "iter 3530: loss 0.9424, time 531.49ms, mfu 0.70%\n",
            "iter 3540: loss 0.9498, time 531.91ms, mfu 0.70%\n",
            "iter 3550: loss 0.9291, time 530.20ms, mfu 0.70%\n",
            "iter 3560: loss 0.9164, time 532.63ms, mfu 0.70%\n",
            "iter 3570: loss 0.9378, time 532.53ms, mfu 0.70%\n",
            "iter 3580: loss 0.9274, time 530.35ms, mfu 0.70%\n",
            "iter 3590: loss 0.9471, time 534.04ms, mfu 0.70%\n",
            "iter 3600: loss 0.9444, time 532.12ms, mfu 0.70%\n",
            "iter 3610: loss 0.9245, time 532.04ms, mfu 0.70%\n",
            "iter 3620: loss 0.9474, time 530.78ms, mfu 0.70%\n",
            "iter 3630: loss 0.9119, time 531.64ms, mfu 0.70%\n",
            "iter 3640: loss 0.9205, time 529.73ms, mfu 0.70%\n",
            "iter 3650: loss 0.9251, time 533.24ms, mfu 0.70%\n",
            "iter 3660: loss 0.9096, time 532.42ms, mfu 0.70%\n",
            "iter 3670: loss 0.9278, time 533.76ms, mfu 0.70%\n",
            "iter 3680: loss 0.9223, time 531.32ms, mfu 0.70%\n",
            "iter 3690: loss 0.9175, time 532.97ms, mfu 0.70%\n",
            "iter 3700: loss 0.9061, time 531.37ms, mfu 0.70%\n",
            "iter 3710: loss 0.9083, time 532.15ms, mfu 0.70%\n",
            "iter 3720: loss 0.9160, time 531.24ms, mfu 0.70%\n",
            "iter 3730: loss 0.9195, time 532.29ms, mfu 0.70%\n",
            "iter 3740: loss 0.9121, time 532.37ms, mfu 0.70%\n",
            "iter 3750: loss 0.8900, time 532.60ms, mfu 0.70%\n",
            "iter 3760: loss 0.9009, time 532.19ms, mfu 0.70%\n",
            "iter 3770: loss 0.9080, time 530.98ms, mfu 0.70%\n",
            "iter 3780: loss 0.9177, time 533.27ms, mfu 0.70%\n",
            "iter 3790: loss 0.9454, time 531.86ms, mfu 0.70%\n",
            "iter 3800: loss 0.9084, time 535.14ms, mfu 0.70%\n",
            "iter 3810: loss 0.8997, time 531.84ms, mfu 0.70%\n",
            "iter 3820: loss 0.8966, time 531.30ms, mfu 0.70%\n",
            "iter 3830: loss 0.9134, time 531.89ms, mfu 0.70%\n",
            "iter 3840: loss 0.9165, time 533.85ms, mfu 0.70%\n",
            "iter 3850: loss 0.8966, time 531.09ms, mfu 0.70%\n",
            "iter 3860: loss 0.9070, time 532.23ms, mfu 0.70%\n",
            "iter 3870: loss 0.9003, time 530.43ms, mfu 0.70%\n",
            "iter 3880: loss 0.9222, time 531.60ms, mfu 0.70%\n",
            "iter 3890: loss 0.8930, time 531.70ms, mfu 0.70%\n",
            "iter 3900: loss 0.8990, time 529.40ms, mfu 0.70%\n",
            "iter 3910: loss 0.8957, time 530.07ms, mfu 0.70%\n",
            "iter 3920: loss 0.8968, time 529.98ms, mfu 0.70%\n",
            "iter 3930: loss 0.8940, time 530.42ms, mfu 0.70%\n",
            "iter 3940: loss 0.8679, time 529.12ms, mfu 0.70%\n",
            "iter 3950: loss 0.8978, time 530.56ms, mfu 0.70%\n",
            "iter 3960: loss 0.8894, time 532.52ms, mfu 0.70%\n",
            "iter 3970: loss 0.9050, time 530.23ms, mfu 0.70%\n",
            "iter 3980: loss 0.8724, time 532.15ms, mfu 0.70%\n",
            "iter 3990: loss 0.8736, time 530.72ms, mfu 0.70%\n",
            "iter 4000: loss 0.8910, time 531.09ms, mfu 0.70%\n",
            "iter 4010: loss 0.9043, time 528.20ms, mfu 0.70%\n",
            "iter 4020: loss 0.8600, time 529.58ms, mfu 0.70%\n",
            "iter 4030: loss 0.8826, time 531.34ms, mfu 0.70%\n",
            "iter 4040: loss 0.8809, time 529.70ms, mfu 0.70%\n",
            "iter 4050: loss 0.8707, time 531.92ms, mfu 0.70%\n",
            "iter 4060: loss 0.8877, time 533.44ms, mfu 0.70%\n",
            "iter 4070: loss 0.8852, time 533.87ms, mfu 0.70%\n",
            "iter 4080: loss 0.8652, time 532.99ms, mfu 0.70%\n",
            "iter 4090: loss 0.8893, time 532.73ms, mfu 0.70%\n",
            "iter 4100: loss 0.8691, time 534.25ms, mfu 0.70%\n",
            "iter 4110: loss 0.8638, time 534.44ms, mfu 0.70%\n",
            "iter 4120: loss 0.8651, time 532.18ms, mfu 0.70%\n",
            "iter 4130: loss 0.8830, time 529.64ms, mfu 0.70%\n",
            "iter 4140: loss 0.8446, time 531.23ms, mfu 0.70%\n",
            "iter 4150: loss 0.8652, time 532.11ms, mfu 0.70%\n",
            "iter 4160: loss 0.8587, time 528.50ms, mfu 0.70%\n",
            "iter 4170: loss 0.8799, time 531.86ms, mfu 0.70%\n",
            "iter 4180: loss 0.8545, time 530.87ms, mfu 0.70%\n",
            "iter 4190: loss 0.8711, time 531.31ms, mfu 0.70%\n",
            "iter 4200: loss 0.8617, time 530.10ms, mfu 0.70%\n",
            "iter 4210: loss 0.8578, time 529.97ms, mfu 0.70%\n",
            "iter 4220: loss 0.8631, time 531.28ms, mfu 0.70%\n",
            "iter 4230: loss 0.8678, time 530.90ms, mfu 0.70%\n",
            "iter 4240: loss 0.8867, time 530.05ms, mfu 0.70%\n",
            "iter 4250: loss 0.8871, time 534.01ms, mfu 0.70%\n",
            "iter 4260: loss 0.8455, time 532.59ms, mfu 0.70%\n",
            "iter 4270: loss 0.8772, time 533.89ms, mfu 0.70%\n",
            "iter 4280: loss 0.8752, time 534.60ms, mfu 0.70%\n",
            "iter 4290: loss 0.8537, time 531.90ms, mfu 0.70%\n",
            "iter 4300: loss 0.8519, time 533.94ms, mfu 0.70%\n",
            "iter 4310: loss 0.8764, time 533.31ms, mfu 0.70%\n",
            "iter 4320: loss 0.8622, time 532.10ms, mfu 0.70%\n",
            "iter 4330: loss 0.8590, time 530.89ms, mfu 0.70%\n",
            "iter 4340: loss 0.8869, time 530.23ms, mfu 0.70%\n",
            "iter 4350: loss 0.8547, time 532.89ms, mfu 0.70%\n",
            "iter 4360: loss 0.8749, time 531.76ms, mfu 0.70%\n",
            "iter 4370: loss 0.8807, time 532.18ms, mfu 0.70%\n",
            "iter 4380: loss 0.8533, time 530.92ms, mfu 0.70%\n",
            "iter 4390: loss 0.8529, time 533.01ms, mfu 0.70%\n",
            "iter 4400: loss 0.8559, time 530.82ms, mfu 0.70%\n",
            "iter 4410: loss 0.8483, time 531.06ms, mfu 0.70%\n",
            "iter 4420: loss 0.8652, time 530.39ms, mfu 0.70%\n",
            "iter 4430: loss 0.8622, time 531.50ms, mfu 0.70%\n",
            "iter 4440: loss 0.8630, time 529.99ms, mfu 0.70%\n",
            "iter 4450: loss 0.8299, time 530.75ms, mfu 0.70%\n",
            "iter 4460: loss 0.8563, time 529.30ms, mfu 0.70%\n",
            "iter 4470: loss 0.8282, time 529.57ms, mfu 0.70%\n",
            "iter 4480: loss 0.8420, time 528.99ms, mfu 0.70%\n",
            "iter 4490: loss 0.8496, time 529.49ms, mfu 0.70%\n",
            "iter 4500: loss 0.8637, time 530.10ms, mfu 0.70%\n",
            "iter 4510: loss 0.8662, time 531.92ms, mfu 0.70%\n",
            "iter 4520: loss 0.8627, time 531.44ms, mfu 0.70%\n",
            "iter 4530: loss 0.8426, time 532.76ms, mfu 0.70%\n",
            "iter 4540: loss 0.8875, time 531.16ms, mfu 0.70%\n",
            "iter 4550: loss 0.8500, time 531.05ms, mfu 0.70%\n",
            "iter 4560: loss 0.8391, time 533.34ms, mfu 0.70%\n",
            "iter 4570: loss 0.8575, time 531.41ms, mfu 0.70%\n",
            "iter 4580: loss 0.8440, time 532.27ms, mfu 0.70%\n",
            "iter 4590: loss 0.8476, time 533.16ms, mfu 0.70%\n",
            "iter 4600: loss 0.8384, time 531.56ms, mfu 0.70%\n",
            "iter 4610: loss 0.8345, time 531.57ms, mfu 0.70%\n",
            "iter 4620: loss 0.8392, time 531.53ms, mfu 0.70%\n",
            "iter 4630: loss 0.8408, time 530.62ms, mfu 0.70%\n",
            "iter 4640: loss 0.8299, time 531.45ms, mfu 0.70%\n",
            "iter 4650: loss 0.8314, time 532.49ms, mfu 0.70%\n",
            "iter 4660: loss 0.8486, time 532.31ms, mfu 0.70%\n",
            "iter 4670: loss 0.8371, time 534.20ms, mfu 0.70%\n",
            "iter 4680: loss 0.8481, time 532.79ms, mfu 0.70%\n",
            "iter 4690: loss 0.8320, time 529.77ms, mfu 0.70%\n",
            "iter 4700: loss 0.8102, time 535.08ms, mfu 0.70%\n",
            "iter 4710: loss 0.8413, time 532.66ms, mfu 0.70%\n",
            "iter 4720: loss 0.8593, time 533.30ms, mfu 0.70%\n",
            "iter 4730: loss 0.8153, time 532.34ms, mfu 0.70%\n",
            "iter 4740: loss 0.8187, time 533.12ms, mfu 0.70%\n",
            "iter 4750: loss 0.8156, time 531.20ms, mfu 0.70%\n",
            "iter 4760: loss 0.8320, time 530.69ms, mfu 0.70%\n",
            "iter 4770: loss 0.8382, time 529.93ms, mfu 0.70%\n",
            "iter 4780: loss 0.8313, time 533.07ms, mfu 0.70%\n",
            "iter 4790: loss 0.8220, time 532.36ms, mfu 0.70%\n",
            "iter 4800: loss 0.8166, time 532.52ms, mfu 0.70%\n",
            "iter 4810: loss 0.8263, time 530.92ms, mfu 0.70%\n",
            "iter 4820: loss 0.8189, time 533.47ms, mfu 0.70%\n",
            "iter 4830: loss 0.8319, time 532.15ms, mfu 0.70%\n",
            "iter 4840: loss 0.8487, time 532.44ms, mfu 0.70%\n",
            "iter 4850: loss 0.8160, time 531.37ms, mfu 0.70%\n",
            "iter 4860: loss 0.8061, time 531.11ms, mfu 0.70%\n",
            "iter 4870: loss 0.8504, time 530.67ms, mfu 0.70%\n",
            "iter 4880: loss 0.8453, time 528.84ms, mfu 0.70%\n",
            "iter 4890: loss 0.8326, time 529.36ms, mfu 0.70%\n",
            "iter 4900: loss 0.8247, time 530.99ms, mfu 0.70%\n",
            "iter 4910: loss 0.8301, time 532.08ms, mfu 0.70%\n",
            "iter 4920: loss 0.8218, time 532.16ms, mfu 0.70%\n",
            "iter 4930: loss 0.8288, time 532.61ms, mfu 0.70%\n",
            "iter 4940: loss 0.8152, time 528.53ms, mfu 0.70%\n",
            "iter 4950: loss 0.8513, time 531.57ms, mfu 0.70%\n",
            "iter 4960: loss 0.8348, time 531.04ms, mfu 0.70%\n",
            "iter 4970: loss 0.8104, time 531.61ms, mfu 0.70%\n",
            "iter 4980: loss 0.8190, time 531.67ms, mfu 0.70%\n",
            "iter 4990: loss 0.8372, time 533.11ms, mfu 0.70%\n",
            "step 5000: train loss 0.6248, val loss 1.6801\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 5000: loss 0.8148, time 75545.49ms, mfu 0.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaIJtyiEgsH5",
        "outputId": "544033a3-dd5f-4c90-8f7f-6de70b6ac4fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And come, my lord,\n",
            "Straight and you not.\n",
            "\n",
            "ISABELLA:\n",
            "It is bar at him that he that did\n",
            "So meet his purpose to him.\n",
            "\n",
            "ANGELO:\n",
            "\n",
            "ISABELLA:\n",
            "When she did but say 'thwack in this maid:\n",
            "Go the before her my brother live to me.\n",
            "\n",
            "ANGELO:\n",
            "I am sorry that she hath left under you.\n",
            "\n",
            "ISABELLA:\n",
            "I would thought it would the instrument of mine\n",
            "Condemned that would she have done to the sour of her\n",
            "After you love the princes.\n",
            "\n",
            "ISABELLA:\n",
            "Now, as it were a dish of sound that is mine,\n",
            "I desire to move my count\n",
            "---------------\n",
            "\n",
            "Men punish, I shall find them at once again.\n",
            "\n",
            "Second Servingman:\n",
            "You will be here a little: she is confessor, she can learn\n",
            "his mother; which shall be beholdered in Carthack\n",
            "be considered.\n",
            "\n",
            "First Servingman:\n",
            "And so did I.\n",
            "\n",
            "Third Servingman:\n",
            "My voice is grown made by the warlike sweet sorrow.\n",
            "\n",
            "Third Servingman:\n",
            "What, is he?\n",
            "\n",
            "Third Servingman:\n",
            "Here hath none but this, but to this once reign in him.\n",
            "\n",
            "First Servingman:\n",
            "Whither?\n",
            "\n",
            "Third Servingman:\n",
            "A greater six from the time would glean for the\n",
            "groun\n",
            "---------------\n",
            "\n",
            "Men talk of weeping in the stocks of heaven\n",
            "And the princess of an oath.\n",
            "\n",
            "CLAUDIO:\n",
            "I have seen the world nor singing,\n",
            "But I have deserved no charity.\n",
            "Come, bloody I have a catalogue,\n",
            "You do bend mine armour only for this excellent.\n",
            "\n",
            "ISABELLA:\n",
            "I would it too detect for Henry's mouth:\n",
            "But your beauty cannot be more violent.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sir, that I may be brief than that you can do it.\n",
            "\n",
            "ISABELLA:\n",
            "None, but that I know.\n",
            "\n",
            "ISABELLA:\n",
            "That will be a wise men of much grief so in a\n",
            "man as any leasy, \n",
            "---------------\n",
            "\n",
            "The slavish my house and cries, even to the deed.\n",
            "\n",
            "GLOUCESTER:\n",
            "By courtesy, I beseech your grace then,\n",
            "Which makes me dry not only promise.\n",
            "When we may be absolute; since is it not?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Then in the thing to command, I'll not find\n",
            "Stand by the heir of their attempt.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Sweet lords, as we say, I remember it,\n",
            "By sovereign and by the root of the world,\n",
            "Somewhate'er our complots in pawn and reign.\n",
            "\n",
            "WARWICK:\n",
            "How, Clifford! for what is thy news?\n",
            "\n",
            "Messenger:\n",
            "My lord, I wonder had \n",
            "---------------\n",
            "\n",
            "Be even but then, if I know thee in thy life,\n",
            "I was an ill faults; and, but the enemy\n",
            "Hath committed thee, or a worthily lamb.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Hold, holy friar, thou must show thee here.\n",
            "\n",
            "ROMEO:\n",
            "Tybalt, then to have wings the end:\n",
            "But the lamentation of such shows on thy master\n",
            "Rush'd with death a fair philosophy; but a rival in\n",
            "Whose beary lives heard his happiness, behind, and the\n",
            "soft rebellion of a man to be shorten'd, the fine--\n",
            "The oracle is all:--how must I self away?\n",
            "\n",
            "HERMIONE:\n",
            "What is \n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He's a leaven's end.\n",
            "\n",
            "BRUTUS:\n",
            "Well, well, well, he was ever framed to the\n",
            "commonwealth that found the good will on the\n",
            "hand before the common.\n",
            "\n",
            "SICINIUS:\n",
            "The senate, Coriolanus.\n",
            "\n",
            "BRUTUS:\n",
            "The second that we have well convented to Rome,\n",
            "Whose services are disposition and hope\n",
            "To mock upon a table fellow of graves.\n",
            "\n",
            "MENENIUS:\n",
            "What then?\n",
            "\n",
            "BRUTUS:\n",
            "The consul Coriolanus\n",
            "Seless in Aufidius that way your highness' east,\n",
            "That you shall hape him by the grave water: for his general\n",
            "Was nothing t\n",
            "---------------\n",
            "\n",
            "She's the beauty of such a weeder,\n",
            "And that she was so; so looks in her liking:\n",
            "She was not so: although I had found in storm,\n",
            "Nor never was nor passes true till thee growing\n",
            "Here in my lord's part: and so if I stand not,\n",
            "But by my redouble profane, I'll change the prince,\n",
            "That she doth permit at me and pluck my paper.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What fates and round them shall ne'er be;\n",
            "For I am going with words to be much fear.\n",
            "\n",
            "LORD ROSS:\n",
            "Lords, what can you call him in that true court?\n",
            "\n",
            "HENRY BOLING\n",
            "---------------\n",
            "\n",
            "lady, when men are not first this business.\n",
            "\n",
            "Second Murderer:\n",
            "I am a party to death.\n",
            "\n",
            "CLARENCE:\n",
            "The boy we are all friendships and royal persons:\n",
            "The crown and cheerfully and degenerate\n",
            "Thou didst be crowned Richard's royal king!\n",
            "Thereon I come to Richmond, thou hast dead mine,\n",
            "That is it crown'd my way, that thou shalt come down.\n",
            "\n",
            "CLARENCE:\n",
            "My mother rather will you go to the Tower.\n",
            "\n",
            "KING EDWARD IV:\n",
            "But where is the Earl of Wiltshire dead?\n",
            "\n",
            "CLARENCE:\n",
            "There is England's messenger.\n",
            "\n",
            "KING EDWARD I\n",
            "---------------\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "BUCKINGHAM:\n",
            "Have done, my lord.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Brother, we bid yourself to-morrow.\n",
            "\n",
            "LADY GREY:\n",
            "Myself are too resolved to be in talk:\n",
            "The little lords be more an ungovern'd blood.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Send them such a tedious friends:\n",
            "Now so wise come to England's majesty\n",
            "With further he that in no momentary which\n",
            "You should forget and have my blessing.\n",
            "\n",
            "KING HENRY VI:\n",
            "Cousin, then, to my short we say; yea, remember it.\n",
            "What then? what's yonder grew of that name?\n",
            "\n",
            "KING HENRY VI:\n",
            "What \n",
            "---------------\n",
            "\n",
            "How cheap is it, rogue, being rapier!\n",
            "\n",
            "BENVOLIO:\n",
            "Not how is it well strange?\n",
            "\n",
            "MERCUTIO:\n",
            "The axe you have stol'n when you have look'd on.\n",
            "\n",
            "BENVOLIO:\n",
            "Still I be merry, and spotless.\n",
            "\n",
            "MERCUTIO:\n",
            "Now, by my charity, is a sudden truth:\n",
            "I am a-wise, a virtuous horse.\n",
            "\n",
            "BENVOLIO:\n",
            "I pray thee, captain and a score,\n",
            "Locking in a gentleman to enfranchisement,\n",
            "A trembling and damnation.\n",
            "\n",
            "MERCUTIO:\n",
            "O, peace!\n",
            "\n",
            "MERCUTIO:\n",
            "Come you not being then, though I had rather\n",
            "Should choose be honest and you hurt into my re\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Model Architecture Exploration (Revised for n_head=2,3,5,7 with n_embd=210, max_iters=1000)\n",
        "# Run in Google Colab with T4 GPU\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 3.1: Train models with Layers=7, Heads=[2, 3, 5, 7], n_embd=210, max_iters=1000\n",
        "heads = [2, 3, 5, 7]\n",
        "for h in heads:\n",
        "    print(f\"Training with Layers=7, Heads={h}\")\n",
        "    cmd = (f\"python train.py config/train_shakespeare_char.py \"\n",
        "           f\"--n_layer=7 --n_head={h} --n_embd=210 --compile=False \"\n",
        "           f\"--out_dir=out-shakespeare-l7-h{h} --max_iters=1000 --batch_size=8 \"\n",
        "           f\"> output_l7_h{h}.txt 2> error_l7_h{h}.txt\")\n",
        "    os.system(cmd)\n",
        "\n",
        "# Debug: Check output and error files\n",
        "for h in heads:\n",
        "    # Check output file\n",
        "    output_file = f\"output_l7_h{h}.txt\"\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            content = f.read().strip()\n",
        "            if content:\n",
        "                print(f\"{output_file} has content (first 100 chars): {content[:100]}\")\n",
        "                last_lines = content.split('\\n')[-10:]\n",
        "                print(f\"Last 10 lines of {output_file}:\\n\", '\\n'.join(last_lines))\n",
        "            else:\n",
        "                print(f\"{output_file} is empty\")\n",
        "    else:\n",
        "        print(f\"{output_file} does not exist\")\n",
        "\n",
        "    # Check error file\n",
        "    error_file = f\"error_l7_h{h}.txt\"\n",
        "    if os.path.exists(error_file):\n",
        "        with open(error_file, 'r') as f:\n",
        "            error_content = f.read().strip()\n",
        "            if error_content:\n",
        "                print(f\"{error_file} has content:\\n{error_content}\")\n",
        "            else:\n",
        "                print(f\"{error_file} is empty\")\n",
        "    else:\n",
        "        print(f\"{error_file} does not exist\")\n",
        "\n",
        "# Step 3.2: Extract losses at iteration 1000\n",
        "def extract_losses(filename, target_iter=1000):\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                if f'step {target_iter}' in line:\n",
        "                    parts = line.split()\n",
        "                    train_loss = float(parts[4].strip(','))  # e.g., \"1.2345,\"\n",
        "                    val_loss = float(parts[7])               # e.g., \"1.3456\"\n",
        "                    return train_loss, val_loss\n",
        "        print(f\"No step {target_iter} found in {filename}\")\n",
        "        return None, None\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {filename} not found\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {filename}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Collect losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for h in heads:  # Corrected from 'heads26 heads'\n",
        "    t_loss, v_loss = extract_losses(f\"output_l7_h{h}.txt\", target_iter=1000)\n",
        "    if t_loss is not None and v_loss is not None:\n",
        "        train_losses.append(t_loss)\n",
        "        val_losses.append(v_loss)\n",
        "    else:\n",
        "        train_losses.append(float('inf'))\n",
        "        val_losses.append(float('inf'))\n",
        "\n",
        "print(\"Heads:\", heads)\n",
        "print(\"Train Losses:\", train_losses)\n",
        "print(\"Val Losses:\", val_losses)\n",
        "\n",
        "# Create figures directory and plot\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "\n",
        "plt.plot(heads, train_losses, marker='o')\n",
        "plt.xlabel('Number of Heads')\n",
        "plt.ylabel('Training Loss at Iteration 1000')\n",
        "plt.title('Training Loss vs. Number of Heads (Layers = 7)')\n",
        "plt.grid(True)\n",
        "plt.savefig('figures/loss_vs_heads.png')\n",
        "plt.close()\n",
        "\n",
        "# Step 3.3: Report lowest validation loss and settings\n",
        "min_val_loss = min(val_losses)\n",
        "best_head = heads[val_losses.index(min_val_loss)] if min_val_loss != float('inf') else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5lG0WHhVV9",
        "outputId": "7ad76437-f79b-4379-ca28-55090c711eb1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Layers=7, Heads=2\n",
            "Training with Layers=7, Heads=3\n",
            "Training with Layers=7, Heads=5\n",
            "Training with Layers=7, Heads=7\n",
            "output_l7_h2.txt has content (first 100 chars): Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakesp\n",
            "Last 10 lines of output_l7_h2.txt:\n",
            " iter 930: loss 2.2269, time 34.31ms, mfu 0.53%\n",
            "iter 940: loss 2.1792, time 34.10ms, mfu 0.53%\n",
            "iter 950: loss 2.2274, time 34.40ms, mfu 0.53%\n",
            "iter 960: loss 2.2008, time 34.52ms, mfu 0.53%\n",
            "iter 970: loss 2.2091, time 34.53ms, mfu 0.52%\n",
            "iter 980: loss 2.1761, time 34.33ms, mfu 0.52%\n",
            "iter 990: loss 2.2278, time 34.62ms, mfu 0.52%\n",
            "step 1000: train loss 2.1299, val loss 2.1881\n",
            "saving checkpoint to out-shakespeare-l7-h2\n",
            "iter 1000: loss 2.2563, time 5258.34ms, mfu 0.47%\n",
            "error_l7_h2.txt has content:\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "output_l7_h3.txt has content (first 100 chars): Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakesp\n",
            "Last 10 lines of output_l7_h3.txt:\n",
            " iter 930: loss 2.1941, time 38.50ms, mfu 0.45%\n",
            "iter 940: loss 2.1753, time 38.59ms, mfu 0.45%\n",
            "iter 950: loss 2.1963, time 36.59ms, mfu 0.46%\n",
            "iter 960: loss 2.1626, time 39.07ms, mfu 0.46%\n",
            "iter 970: loss 2.2025, time 37.89ms, mfu 0.46%\n",
            "iter 980: loss 2.1507, time 38.02ms, mfu 0.46%\n",
            "iter 990: loss 2.1979, time 38.15ms, mfu 0.46%\n",
            "step 1000: train loss 2.1136, val loss 2.1665\n",
            "saving checkpoint to out-shakespeare-l7-h3\n",
            "iter 1000: loss 2.2220, time 5854.53ms, mfu 0.41%\n",
            "error_l7_h3.txt has content:\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "output_l7_h5.txt has content (first 100 chars): Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakesp\n",
            "Last 10 lines of output_l7_h5.txt:\n",
            " iter 930: loss 2.1624, time 39.79ms, mfu 0.43%\n",
            "iter 940: loss 2.1357, time 40.12ms, mfu 0.43%\n",
            "iter 950: loss 2.1720, time 40.63ms, mfu 0.43%\n",
            "iter 960: loss 2.1574, time 40.19ms, mfu 0.44%\n",
            "iter 970: loss 2.1533, time 40.14ms, mfu 0.44%\n",
            "iter 980: loss 2.1169, time 40.52ms, mfu 0.44%\n",
            "iter 990: loss 2.1776, time 39.95ms, mfu 0.44%\n",
            "step 1000: train loss 2.0735, val loss 2.1388\n",
            "saving checkpoint to out-shakespeare-l7-h5\n",
            "iter 1000: loss 2.1939, time 6355.71ms, mfu 0.39%\n",
            "error_l7_h5.txt has content:\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "output_l7_h7.txt has content (first 100 chars): Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakesp\n",
            "Last 10 lines of output_l7_h7.txt:\n",
            " iter 930: loss 2.1666, time 43.74ms, mfu 0.39%\n",
            "iter 940: loss 2.1289, time 44.10ms, mfu 0.39%\n",
            "iter 950: loss 2.1626, time 43.58ms, mfu 0.40%\n",
            "iter 960: loss 2.1353, time 43.71ms, mfu 0.40%\n",
            "iter 970: loss 2.1708, time 43.66ms, mfu 0.40%\n",
            "iter 980: loss 2.1286, time 43.97ms, mfu 0.40%\n",
            "iter 990: loss 2.1551, time 44.62ms, mfu 0.40%\n",
            "step 1000: train loss 2.0775, val loss 2.1440\n",
            "saving checkpoint to out-shakespeare-l7-h7\n",
            "iter 1000: loss 2.1946, time 7009.74ms, mfu 0.36%\n",
            "error_l7_h7.txt has content:\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Heads: [2, 3, 5, 7]\n",
            "Train Losses: [2.1299, 2.1136, 2.0735, 2.0775]\n",
            "Val Losses: [2.1881, 2.1665, 2.1388, 2.144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create data/code_generation directory by copying data/shakespeare_char\n",
        "os.makedirs('data/code_generation', exist_ok=True)\n",
        "shutil.copy('data/shakespeare_char/prepare.py', 'data/code_generation/prepare.py')\n",
        "\n",
        "# Clone The-Young-Programmer/C-CPP-Programming repository\n",
        "print(\"Cloning The-Young-Programmer/C-CPP-Programming repository...\")\n",
        "os.system('git clone https://github.com/The-Young-Programmer/C-CPP-Programming.git')\n",
        "\n",
        "# Aggregate all .c, .cpp, and .h files into input.txt\n",
        "output_file = 'data/code_generation/input.txt'\n",
        "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for root, dirs, files in os.walk('C-CPP-Programming'):\n",
        "        for file in files:\n",
        "            if file.endswith(('.c', '.cpp', '.h')):  # Include C, C++, and header files\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "                        outfile.write(infile.read())\n",
        "                        outfile.write('\\n\\n')  # Separate files\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "# Verify file size (rough estimate: ~4 chars per token)\n",
        "file_size = os.path.getsize(output_file)\n",
        "estimated_tokens = file_size // 4\n",
        "print(f\"Size of input.txt: {file_size} bytes (~{estimated_tokens} tokens)\")\n",
        "\n",
        "# Duplicate content if token count is below 100,000\n",
        "if estimated_tokens < 100000:\n",
        "    print(\"Duplicating input.txt content to meet token requirement...\")\n",
        "    with open(output_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for _ in range((100000 // estimated_tokens) + 1):\n",
        "            f.write(content)\n",
        "            f.write('\\n\\n')\n",
        "    new_size = os.path.getsize(output_file)\n",
        "    estimated_tokens = new_size // 4\n",
        "    print(f\"New size of input.txt: {new_size} bytes (~{estimated_tokens} tokens)\")\n",
        "\n",
        "# Run prepare.py to process the dataset\n",
        "os.chdir('data/code_generation')\n",
        "print(\"Running prepare.py...\")\n",
        "os.system('python prepare.py')\n",
        "os.chdir('../..')\n",
        "\n",
        "# Read vocab_size and token count from meta.pkl\n",
        "import numpy as np\n",
        "import pickle\n",
        "meta_path = 'data/code_generation/meta.pkl'\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    vocab_size = meta['vocab_size']\n",
        "    train_bin = np.memmap('data/code_generation/train.bin', dtype=np.uint16, mode='r')\n",
        "    token_count = len(train_bin)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    print(f\"Token count: {token_count}\")\n",
        "else:\n",
        "    print(\"meta.pkl not found. Check prepare.py output.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BTjM4nHiv6p",
        "outputId": "fc724286-3440-474d-fabf-f9a383ffa6b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning The-Young-Programmer/C-CPP-Programming repository...\n",
            "Size of input.txt: 132431 bytes (~33107 tokens)\n",
            "Duplicating input.txt content to meet token requirement...\n",
            "New size of input.txt: 529732 bytes (~132433 tokens)\n",
            "Running prepare.py...\n",
            "Vocab size: 111\n",
            "Token count: 476445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Train the model\n",
        "print(\"Training BabyGPT on C/C++ code generation dataset...\")\n",
        "start_time = time.time()\n",
        "cmd = (f\"python train.py config/train_code_generation.py \"\n",
        "       f\"--compile=False --out_dir=out-code-generation \"\n",
        "       f\"> output_code_generation.txt 2> error_code_generation.txt\")\n",
        "os.system(cmd)\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training completed in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Check output file\n",
        "output_file = 'output_code_generation.txt'\n",
        "if os.path.exists(output_file):\n",
        "    with open(output_file, 'r') as f:\n",
        "        content = f.read().strip()\n",
        "        if content:\n",
        "            print(f\"{output_file} has content (first 100 chars): {content[:100]}\")\n",
        "            last_lines = content.split('\\n')[-10:]\n",
        "            print(f\"Last 10 lines of {output_file}:\\n\", '\\n'.join(last_lines))\n",
        "        else:\n",
        "            print(f\"{output_file} is empty\")\n",
        "else:\n",
        "    print(f\"{output_file} does not exist\")\n",
        "\n",
        "# Check error file\n",
        "error_file = 'error_code_generation.txt'\n",
        "if os.path.exists(error_file):\n",
        "    with open(error_file, 'r') as f:\n",
        "        error_content = f.read().strip()\n",
        "        if error_content:\n",
        "            print(f\"{error_file} has content:\\n{error_content}\")\n",
        "        else:\n",
        "            print(f\"{error_file} is empty\")\n",
        "else:\n",
        "    print(f\"{error_file} does not exist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtMcVnb1ixhN",
        "outputId": "6c6d77a5-79c2-43bf-cfc4-fab81fa4aa47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BabyGPT on C/C++ code generation dataset...\n",
            "Training completed in 402.17 seconds\n",
            "output_code_generation.txt has content (first 100 chars): Overriding config with config/train_code_generation.py:\n",
            "# config/train_code_generation.py\n",
            "out_dir = \n",
            "Last 10 lines of output_code_generation.txt:\n",
            " iter 430: loss 0.6991, time 748.35ms, mfu 0.05%\n",
            "iter 440: loss 1.8041, time 741.91ms, mfu 0.05%\n",
            "iter 450: loss 1.5769, time 746.20ms, mfu 0.05%\n",
            "iter 460: loss 1.2985, time 754.58ms, mfu 0.05%\n",
            "iter 470: loss 1.2101, time 741.13ms, mfu 0.05%\n",
            "iter 480: loss 0.6765, time 719.46ms, mfu 0.05%\n",
            "iter 490: loss 1.6352, time 729.79ms, mfu 0.05%\n",
            "step 500: train loss 1.0083, val loss 0.9693\n",
            "saving checkpoint to out-code-generation\n",
            "iter 500: loss 0.7077, time 3690.31ms, mfu 0.05%\n",
            "error_code_generation.txt has content:\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Generate samples\n",
        "print(\"Generating samples from C/C++ code generation model...\")\n",
        "cmd = (f\"python sample.py --out_dir=out-code-generation --start='def ' \"\n",
        "       f\"> samples_code_generation.txt 2> error_samples_code_generation.txt\")\n",
        "os.system(cmd)\n",
        "\n",
        "# Read the first 20 lines of the generated samples\n",
        "sample_file = 'samples_code_generation.txt'\n",
        "if os.path.exists(sample_file):\n",
        "    with open(sample_file, 'r') as f:\n",
        "        samples = f.readlines()\n",
        "        samples = [line.strip() for line in samples if line.strip()]\n",
        "        first_20_lines = samples[:20]\n",
        "        print(\"First 20 lines of generated samples:\")\n",
        "        for line in first_20_lines:\n",
        "            print(line)\n",
        "else:\n",
        "    first_20_lines = [\"No samples generated. Check error_samples_code_generation.txt.\"]\n",
        "    print(f\"{sample_file} does not exist\")\n",
        "\n",
        "# Check error file\n",
        "error_sample_file = 'error_samples_code_generation.txt'\n",
        "if os.path.exists(error_sample_file):\n",
        "    with open(error_sample_file, 'r') as f:\n",
        "        error_content = f.read().strip()\n",
        "        if error_content:\n",
        "            print(f\"{error_sample_file} has content:\\n{error_content}\")\n",
        "        else:\n",
        "            print(f\"{error_sample_file} is empty\")\n",
        "else:\n",
        "    print(f\"{error_sample_file} does not exist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gmyS8LGlAQd",
        "outputId": "39a53a13-ee84-48f1-fb4f-da3b314d3f00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating samples from C/C++ code generation model...\n",
            "First 20 lines of generated samples:\n",
            "Overriding: out_dir = out-code-generation\n",
            "Overriding: start = def\n",
            "number of parameters: 3.73M\n",
            "Loading meta from data/code_generation/meta.pkl...\n",
            "def boardace\n",
            "case '\\n';\n",
            "return ch;\n",
            "}\n",
            "// colord candition to din tition\n",
            "// allt_scording();\n",
            "{\n",
            "cout << \"\\t\\t3.Restetext\\t\";\n",
            "gotoxy(row, col);\n",
            "cout << \"\\t\\t Presssss Ad\\n^-----------------------------------------------------------------------------------------------------------------------\n",
            "<< \"\\t\\t            \"\n",
            "<< \"\\t\\t\\t\\t\"\n",
            "< \"\\t\\t\\t\\t\\t                 \"\n",
            "<< '\\n'\n",
            "<<\"\\n\\t\\t  \"\n",
            "---------------\n",
            "error_samples_code_generation.txt is empty\n"
          ]
        }
      ]
    }
  ]
}